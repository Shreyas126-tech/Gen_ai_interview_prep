1. What do you understand by Machine Learning (ML) and how does it differ from artificial intelligence (AI) and Data Science?
Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to learn and make predictions or decisions without being explicitly programmed. ML allows systems to improve their performance on a specific task over time as they are exposed to more data.

Underfitting: It occurs when a model is too simple to capture the underlying patterns in the data. This leads to poor accuracy on both training and test data.
Overfitting: It occurs when a model is too complex and captures noise in the training data as if it were a true pattern. This leads to high accuracy on training data but poor generalization to new, unseen data.

 What is Regularization?
Regularization is a technique used to prevent overfitting in machine learning models by adding a penalty term to the loss function. This penalty discourages the model from fitting the noise in the training data and encourages it to find a simpler, more generalizable solution. Common regularization techniques include L1 regularization (Lasso), which adds a penalty proportional to the absolute value of the coefficients, and L2 regularization (Ridge), which adds a penalty proportional to the square of the coefficients. Regularization helps improve the model's performance on unseen data by reducing its complexity.

 Explain Lasso and Ridge Regularization. How do they help in Elastic Net Regularization?
Lasso Regularization (L1) adds a penalty term to the loss function that is proportional to the absolute value of the coefficients. This can lead to some coefficients being exactly zero, effectively performing feature selection by eliminating less important features from the model.
Ridge Regularization (L2) adds a penalty term to the loss function that is proportional to the square of the coefficients. This encourages the model to keep all features but reduces the magnitude of the coefficients, which can help prevent overfitting.
Elastic Net Regularization is a combination of Lasso and Ridge regularization. It adds both L1 and L2 penalties to the loss function, allowing it to perform feature selection while also shrinking the coefficients. This can be particularly useful when there are multiple correlated features in the dataset, as it can help to select one of them while still keeping the others in the model. Elastic Net Regularization provides a more flexible approach to regularization by allowing the user to balance between Lasso and Ridge penalties based on the specific characteristics of the data.

What are different Model Evaluation Techniques in Machine Learning?
There are several model evaluation techniques in machine learning, including:
- Train-Test Split: The dataset is split into a training set and a test set. The model is trained on the training set and evaluated on the test set to assess its performance.
- Cross-Validation: The dataset is divided into k subsets (folds). The model is trained on k-1 folds and evaluated on the remaining fold. This process is repeated k times, with each fold serving as the test set once. The average performance across all folds is used as the final evaluation metric.
- Confusion Matrix: A table that summarizes the performance of a classification model by showing the true positives, true negatives, false positives, and false negatives. It helps to calculate metrics such as accuracy, precision, recall, and F1-score.
- ROC Curve and AUC: The Receiver Operating Characteristic (ROC) curve plots the true positive rate against the false positive rate at various threshold settings. The Area Under the Curve (AUC) provides a single metric to evaluate the model's performance, with higher values indicating better performance.
- Precision-Recall Curve: This curve plots precision against recall for different threshold values. It is particularly useful for imbalanced datasets where the positive class is rare.
- Mean Absolute Error (MAE): A metric used for regression tasks that measures the average absolute difference between the predicted and actual values.
- Mean Squared Error (MSE): A metric used for regression tasks that measures the average squared difference between the predicted and actual values. It gives more weight to larger errors compared to MAE.
- R-squared: A metric used for regression tasks that indicates the proportion of the variance in    the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with higher values indicating better model performance.
